{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":37705,"sourceType":"datasetVersion","datasetId":29561}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/candacevogel/celeba-dataset-candace-vogel?scriptVersionId=263059595\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CelebA Glasses Facial Characteristic Recognition","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset, random_split\nimport torchvision.transforms.v2 as transforms\nfrom torchvision.transforms import ToTensor\nfrom torchvision import datasets\n\nfrom os import path\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T00:18:58.081916Z","iopub.execute_input":"2025-09-21T00:18:58.083094Z","iopub.status.idle":"2025-09-21T00:18:58.088442Z","shell.execute_reply.started":"2025-09-21T00:18:58.082942Z","shell.execute_reply":"2025-09-21T00:18:58.087491Z"}},"outputs":[],"execution_count":233},{"cell_type":"code","source":"# Load data - create path to files","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T00:18:58.090203Z","iopub.execute_input":"2025-09-21T00:18:58.090572Z","iopub.status.idle":"2025-09-21T00:18:58.114355Z","shell.execute_reply.started":"2025-09-21T00:18:58.090539Z","shell.execute_reply":"2025-09-21T00:18:58.113037Z"}},"outputs":[],"execution_count":234},{"cell_type":"code","source":"dataset_root = '/kaggle/input/celeba-dataset'\n\n# input data from dataset\n\nimage_path = path.join(dataset_root, 'img_align_celeba/img_align_celeba')\n# image_path contains the jpg files\nattributes_path = path.join(dataset_root, 'list_attr_celeba.csv')\n# file path for the csv file containing the attributes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T00:18:58.115293Z","iopub.execute_input":"2025-09-21T00:18:58.115581Z","iopub.status.idle":"2025-09-21T00:18:58.138077Z","shell.execute_reply.started":"2025-09-21T00:18:58.115559Z","shell.execute_reply":"2025-09-21T00:18:58.137107Z"}},"outputs":[],"execution_count":235},{"cell_type":"markdown","source":"# Read attributes CSV","metadata":{}},{"cell_type":"code","source":"df_attributes = pd.read_csv(attributes_path)\ndf_attributes.head()\n# dataframe (df) is a type of table that panda library uses\n# read csv file using pandas library ^\ndf_attributes.replace(-1, 0, inplace=True)\ndf_attributes['Eyeglasses'].head()\n\n# this output will print just the eyeglasses data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T00:18:58.139477Z","iopub.execute_input":"2025-09-21T00:18:58.139793Z","iopub.status.idle":"2025-09-21T00:18:58.880358Z","shell.execute_reply.started":"2025-09-21T00:18:58.139763Z","shell.execute_reply":"2025-09-21T00:18:58.879392Z"}},"outputs":[{"execution_count":236,"output_type":"execute_result","data":{"text/plain":"0    0\n1    0\n2    0\n3    0\n4    0\nName: Eyeglasses, dtype: int64"},"metadata":{}}],"execution_count":236},{"cell_type":"code","source":"# Replace -1 with 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T00:18:58.883068Z","iopub.execute_input":"2025-09-21T00:18:58.883353Z","iopub.status.idle":"2025-09-21T00:18:58.887245Z","shell.execute_reply.started":"2025-09-21T00:18:58.883329Z","shell.execute_reply":"2025-09-21T00:18:58.886362Z"}},"outputs":[],"execution_count":237},{"cell_type":"code","source":"\ndf_attributes.replace(-1, 0, inplace=True)\n# 1 is true / represents wearing glasses\n# 0 is false / represents not wearing glasses\n# replaces every instance of -1 with 0\n# now the data is all 0s and 1s\ndf_attributes['Eyeglasses'].head()\n# prints just eyeglasses attributes column","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T00:18:58.888472Z","iopub.execute_input":"2025-09-21T00:18:58.88878Z","iopub.status.idle":"2025-09-21T00:18:58.946917Z","shell.execute_reply.started":"2025-09-21T00:18:58.888759Z","shell.execute_reply":"2025-09-21T00:18:58.946131Z"}},"outputs":[{"execution_count":238,"output_type":"execute_result","data":{"text/plain":"0    0\n1    0\n2    0\n3    0\n4    0\nName: Eyeglasses, dtype: int64"},"metadata":{}}],"execution_count":238},{"cell_type":"markdown","source":"# How many people are wearing glasses?","metadata":{}},{"cell_type":"code","source":"df_attributes['Eyeglasses'].value_counts()\n# prints number of people wearing glasses in dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T00:18:58.947742Z","iopub.execute_input":"2025-09-21T00:18:58.948009Z","iopub.status.idle":"2025-09-21T00:18:58.956738Z","shell.execute_reply.started":"2025-09-21T00:18:58.947982Z","shell.execute_reply":"2025-09-21T00:18:58.955793Z"}},"outputs":[{"execution_count":239,"output_type":"execute_result","data":{"text/plain":"Eyeglasses\n0    189406\n1     13193\nName: count, dtype: int64"},"metadata":{}}],"execution_count":239},{"cell_type":"markdown","source":"# Balance dataset","metadata":{}},{"cell_type":"code","source":"glasses_df = df_attributes [ df_attributes ['Eyeglasses'] == 1]\nno_glasses_df = df_attributes [ df_attributes['Eyeglasses'] == 0]\n# filtering attributes for glasses and no glasses\n\nlen(glasses_df), len(no_glasses_df)\n# prints how many people are wearing glasses vs no glasses\nglasses_count = len(glasses_df)\n# prints how many people have glasses \nno_glasses_same_size = no_glasses_df.sample(glasses_count)\n# sample is pandas method (pick rows at random)\nlen(no_glasses_same_size)\n# prints how many people don't wear glasses in dataset\ndf_glasses_training = pd.concat ( [glasses_df, no_glasses_same_size], axis=0 )\n# concatenate variables onto row axis\n# axis=0 stacks tables vertically\n# df_glasses_training is our balanced dataset\n# the data is normalized so there is an equal number of images for glasses vs no glasses\n# len(df_glasses_training)\n\ndf_glasses = df_glasses_training[['image_id','Eyeglasses']].sample(10)\n#display 10 rows at random\ndf_glasses.sample(10)\n#only prints attributes we care about - normalized eyeglasses data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T00:18:58.957621Z","iopub.execute_input":"2025-09-21T00:18:58.9579Z","iopub.status.idle":"2025-09-21T00:18:59.066153Z","shell.execute_reply.started":"2025-09-21T00:18:58.957873Z","shell.execute_reply":"2025-09-21T00:18:59.065216Z"}},"outputs":[{"execution_count":240,"output_type":"execute_result","data":{"text/plain":"          image_id  Eyeglasses\n7231    007232.jpg           1\n74746   074747.jpg           0\n81390   081391.jpg           1\n68077   068078.jpg           0\n194326  194327.jpg           1\n61673   061674.jpg           0\n100939  100940.jpg           1\n50043   050044.jpg           1\n116302  116303.jpg           1\n124670  124671.jpg           0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>Eyeglasses</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7231</th>\n      <td>007232.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>74746</th>\n      <td>074747.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>81390</th>\n      <td>081391.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>68077</th>\n      <td>068078.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>194326</th>\n      <td>194327.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>61673</th>\n      <td>061674.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>100939</th>\n      <td>100940.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>50043</th>\n      <td>050044.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>116302</th>\n      <td>116303.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>124670</th>\n      <td>124671.jpg</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":240},{"cell_type":"markdown","source":"# Create a Dataset Class","metadata":{}},{"cell_type":"code","source":"class CelebDataset(Dataset):\n    def __init__(self, images_path, attributes_dataframe, img_transform=None, attr_transform=None):\n        self.images_path = images_path\n        self.attributes_dataframe = attributes_dataframe\n        self.img_transform = img_transform\n        self.attr_transform = attr_transform\n        self.image_filename = attributes_dataframe['image_id'].tolist()\n\n    def __getitem__(self, index):\n        # return the image and its label that are in position (index) in the dataset\n        image_filename = self.image_filename[index]\n        image_path = path.join(self.images_path, image_filename)\n        img = Image.open(image_path).convert('L')\n        # convert is PIL function - grayscale images require less storage\n\n        attributes = self.attributes_dataframe.iloc[index]\n        glasses = attributes.Eyeglasses.astype('int')\n        # need to process data before we send to NN \n        # if img_transform is provided...\n        \n        if self.img_transform:\n            image = self.img_transform(img)\n            # call img_transform function\n\n        if self.attr_transform:\n            glasses = self.attr_transform(glasses)\n            # not used, butmay be useful in another situation\n\n        return img, glasses\n\n    def __len__(self):\n        return len(self.attributes_dataframe)\n        \n\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T00:18:59.067173Z","iopub.execute_input":"2025-09-21T00:18:59.067447Z","iopub.status.idle":"2025-09-21T00:18:59.076106Z","shell.execute_reply.started":"2025-09-21T00:18:59.067425Z","shell.execute_reply":"2025-09-21T00:18:59.074539Z"}},"outputs":[],"execution_count":241},{"cell_type":"markdown","source":"# Define image transformations","metadata":{}},{"cell_type":"code","source":"image_size = 128\n\nimage_transform = transforms.Compose([\n    transforms.Resize(image_size),\n    # resizes on the shorter edge\n    transforms.CenterCrop( [image_size, image_size] ),\n    ToTensor(),\n])\n\ndataset = CelebDataset(image_path, df_glasses, image_transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T00:18:59.077212Z","iopub.execute_input":"2025-09-21T00:18:59.077464Z","iopub.status.idle":"2025-09-21T00:18:59.102559Z","shell.execute_reply.started":"2025-09-21T00:18:59.077443Z","shell.execute_reply":"2025-09-21T00:18:59.101553Z"}},"outputs":[],"execution_count":242},{"cell_type":"markdown","source":"# Divide dataset into training and test data","metadata":{}},{"cell_type":"code","source":"train_dataset, test_dataset = random_split(dataset, (0.8, 0.2) )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T00:18:59.103678Z","iopub.execute_input":"2025-09-21T00:18:59.103987Z","iopub.status.idle":"2025-09-21T00:18:59.123298Z","shell.execute_reply.started":"2025-09-21T00:18:59.103938Z","shell.execute_reply":"2025-09-21T00:18:59.122114Z"}},"outputs":[],"execution_count":243},{"cell_type":"markdown","source":"# Create dataloaders for train and test datasets","metadata":{}},{"cell_type":"code","source":"batch_size = 32\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T00:18:59.124483Z","iopub.execute_input":"2025-09-21T00:18:59.124787Z","iopub.status.idle":"2025-09-21T00:18:59.145568Z","shell.execute_reply.started":"2025-09-21T00:18:59.124763Z","shell.execute_reply":"2025-09-21T00:18:59.14446Z"}},"outputs":[],"execution_count":244},{"cell_type":"markdown","source":"# Visualize example images","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfigure = plt.figure(figsize=(8,8))\ncols = 3\nrows = 3\nfor i in range(1, cols * rows):\n    sample_index = torch.randint(len(train_dataset), size=(1, )).item()\n    img, label = train_dataset[sample_index]\n    figure.add_subplot(rows, cols, i)\n    plt.title('Glasses' if label == 1 else 'No glasses')\n    plt.axis('off')\n    plt.imshow(img.squeeze(), cmap='gray')\n\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T00:18:59.14687Z","iopub.execute_input":"2025-09-21T00:18:59.147274Z","iopub.status.idle":"2025-09-21T00:18:59.351515Z","shell.execute_reply.started":"2025-09-21T00:18:59.147196Z","shell.execute_reply":"2025-09-21T00:18:59.350152Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3664588010.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Glasses'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'No glasses'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Image' object has no attribute 'squeeze'"],"ename":"AttributeError","evalue":"'Image' object has no attribute 'squeeze'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x800 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":245}]}